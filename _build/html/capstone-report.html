
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Capstone Report</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-2 bd-sidebar site-navigation show single-page" id="site-navigation">
    
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/larahabashy/capstone-report"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/larahabashy/capstone-report/issues/new?title=Issue%20on%20page%20%2Fcapstone-report.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="capstone-report.html#document-documentation">
   Appendix A - Documentation
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="capstone-report.html#document-glossary">
   Appendix B - Glossary
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="final-report">
<h1>Final Report<a class="headerlink" href="#final-report" title="Permalink to this headline">¶</a></h1>
<p><strong>MDS 2021 Capstone Project with the Gerontology Diabetes Research Lab (GDRL)</strong></p>
<p>By: Ela Bandari, Lara Habashy, Javairia Raza and Peter Yang</p>
<div class="section" id="executive-summary">
<h2>1. Executive Summary<a class="headerlink" href="#executive-summary" title="Permalink to this headline">¶</a></h2>
<p>Subclinical lipohypertrophy is traditionally evaluated by visual inspection or palpation. Recent work has shown that lipohypertrophy may be detected by ultrasound imaging <span id="id1">[<a class="reference internal" href="#id26">Kapeluto <em>et al.</em>, 2018</a>]</span>. However, the criteria used to classify lipohypertrophy using ultrasound imaging is only familiar to and implemented by a small group of physicians <span id="id2">[<a class="reference internal" href="#id27">Madden, 2021</a>]</span>. In an effort to improve the accessibility and efficiency of this method of detection, we have developed a supervised machine learning model to detect lipohypertrophy in ultrasound images.</p>
<p>We developed an optimal machine learning model through an iterative process of data augmentation, hyperparameter tuning, and architecture selection. The final optimal model accurately classified 76% of unseen test data. We tested different image augmentation techniques and ultimately decided on adding random flipping, contrast and brightness. We then fit a variety of pre-existing model architectures trained on thousands of images to our augmented dataset. We optimized the parameters by which the model learned and then carefully examined the different models’ performance and fine tuned them to our dataset before selecting the best performing model. We have made our best performing model accessible to potential users via a <a class="reference external" href="https://share.streamlit.io/xudongyang2/lipo_deployment/demo_v2.py">web interface</a>. Our work has demonstrated the potential that supervised machine learning holds in accurately detecting lipohypertrophy; however, the scarcity of the ultrasound images has been a contributor to the model’s shortcomings. We have outlined a set of recommendations (see section <a class="reference internal" href="#recommendations"><span class="std std-ref">6. Conclusions and recommendations</span></a>) for improving this project; the most notable of which is re-fitting the model using a larger dataset.</p>
</div>
<div class="section" id="introduction">
<h2>2. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Subclinical lipohypertrophy is a common complication for diabetic patients who inject insulin. It is defined as the growth of fat cells and fibrous tissue in the deepest layer of the skin following repeated insulin injections in the same area. It is critical that insulin is not injected into areas of lipohypertrophy as it reduces the effectiveness of the insulin <span id="id3">[<a class="reference internal" href="#id26">Kapeluto <em>et al.</em>, 2018</a>]</span>. However, recent research by <span id="id4">Kapeluto <em>et al.</em> [<a class="reference internal" href="#id26">2018</a>]</span> has found ultrasound imaging techniques are more accurate in finding these masses than a physical examination of the body by a healthcare professional. Examples of these images are shown in <a class="reference internal" href="#example"><span class="std std-numref">Fig. 1</span></a> below. Unfortunately, the criteria to classify lipohypertrophy using ultrasound imaging is only implemented by a small group of physicians. To expand the usability of this criteria to a larger set of healthcare professionals, the capstone partner is interested in seeing if we can leverage supervised machine learning techniques to accurately classify the presence of lipohypertrophy given an ultrasound image.</p>
<div class="figure align-default" id="example">
<a class="reference internal image-reference" href="_images/example.png"><img alt="_images/example.png" src="_images/example.png" style="height: 500px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">Some examples of images found in our dataset. Top row is negative (no lipohypertrophy present) and bottom row is positive (lipohypertrophy present) images where the yellow annotations indicate the exact area of the mass.</span><a class="headerlink" href="#example" title="Permalink to this image">¶</a></p>
</div>
<p>Our current dataset includes 218 negative images (no lipohypertrophy present) and 135 positive images (lipohypertrophy present) as shown in <a class="reference internal" href="#counts-df"><span class="std std-numref">Fig. 2</span></a> below. Notably, this is considered to be a very small dataset for a deep learning model.</p>
<p>Our specific data science objectives for this project include:</p>
<ol class="simple">
<li><p>Use the provided dataset to develop and evaluate the efficacy of an image classification model. Some of our specific goals include:</p>
<ol class="simple">
<li><p>Data Augmentation: applying random image transformations as to expland the dataset (section <a class="reference internal" href="#data-prep"><span class="std std-ref">4.1 Data Preparation</span></a>)</p></li>
<li><p>Transfer Learning: using a model that has been pre-trained on thousands of images and using it for our dataset (section <a class="reference internal" href="#model-dev"><span class="std std-ref">4.2 Model Development</span></a>)</p></li>
<li><p>Optimization of Learning: figuring out what are the best parameters that will make the model most optimal for learning the data (section <a class="reference internal" href="#model-dev"><span class="std std-ref">4.2 Model Development</span></a>)</p></li>
<li><p>Object Detection: training a model to detect the location of lipohypertrophy on an image (section <a class="reference internal" href="#obj-detect"><span class="std std-ref">4.3 Object Detection</span></a>)</p></li>
</ol>
</li>
<li><p>Deploy the model for a non-technical audience (section <a class="reference internal" href="#data-product"><span class="std std-ref">5. Data Product and Results</span></a>).</p></li>
</ol>
<div class="figure align-default" id="counts-df">
<a class="reference internal image-reference" href="_images/counts_df.png"><img alt="_images/counts_df.png" src="_images/counts_df.png" style="height: 100px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">The total number of positive and negative examples in our dataset.</span><a class="headerlink" href="#counts-df" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="literature-review">
<h2>3. Literature Review<a class="headerlink" href="#literature-review" title="Permalink to this headline">¶</a></h2>
<p>Throughout the project, the team has consulted literature to gain insight and direction on current practices relevant to our problem and our dataset to guide and validate our decision-making. Below is a summary of the relevant findings.</p>
<div class="section" id="similar-work">
<h3>3.1 Similar Work<a class="headerlink" href="#similar-work" title="Permalink to this headline">¶</a></h3>
<p>Prediction of masses in ultrasound images using machine learning techniques has been an ongoing effort in clinical practice for the past few decades. To assist physicians in diagnosing disease, many scholars have implemented techniques such as regression, decision trees, Naive Bayesian classifiers, and neural networks on patients’ ultrasound imaging data <span id="id5">[<a class="reference internal" href="#id32">Huang <em>et al.</em>, 2018</a>]</span>. Further, many studies involving ultrasound images have preprocessed the images to extract features. This study by <span id="id6">Chiao <em>et al.</em> [<a class="reference internal" href="#id33">2019</a>]</span> shows that CNNs using ultrasound images perform better than radiomic models at predicting breast cancer tumours. Another recent study shows success in classifying liver masses into 1 out 5 categories with 84% accuracy using a CNN model <span id="id7">[<a class="reference internal" href="#id34">Yasaka <em>et al.</em>, 2018</a>]</span>.</p>
<p>Furthermore, recent research has delved into various complex image augmentation approaches such as using GANs to generate images <span id="id8">[<a class="reference internal" href="#id25">Al-Dhabyani <em>et al.</em>, 2019</a>]</span>, enlarging the dataset used for training which naturally improves the performance. The study also found that traditional transformations managed to improve model performance. Many other studies such as <span id="id9">[<a class="reference internal" href="#id31">Esteva <em>et al.</em>, 2017</a>, <a class="reference internal" href="#id28">Loey <em>et al.</em>, 2020</a>]</span> confirmed that minimal transformations such as flipping the images achieved higher prediction accuracy in their application.</p>
<p>We also found that transfer learning architectures are crucial for yielding reliable results as any given patient’s dataset is likely too small to construct a CNN architecture. This study by <span id="id10">Sun <em>et al.</em> [<a class="reference internal" href="#id35">2020</a>]</span> built a CNN using DenseNet models for the prediction of breast cancer masses and achieved a test accuracy of 91%. <span id="id11">Zhang <em>et al.</em> [<a class="reference internal" href="#id36">2020</a>]</span> also studying the diagnosis of breast cancer tumours in ultrasound images found that when compared with other architectures such as VGG16, ResNet50, and VGG19, InceptionV3 performs the best. Prediction competitions on Kaggle have proven successful with architectures such as VGG16 and Resnet, predicting masses in ultrasound images with 70-80% accuracy <span id="id12">[<a class="reference internal" href="#id24">Kaggle, 2016</a>]</span>.</p>
<p>For the object detection piece, along with the YOLO framework, we considered RCNNs but research showed YOLO performs well in Kaggle competitions <span id="id13">[<a class="reference internal" href="#id23">Kaggle, 2021</a>]</span> and is generally faster than RCNNs. As such, we went with the YOLO model <span id="id14">[<a class="reference internal" href="#id22">Gandhi, 2018</a>]</span>.</p>
</div>
</div>
<div class="section" id="data-science-methods">
<h2>4. Data Science Methods<a class="headerlink" href="#data-science-methods" title="Permalink to this headline">¶</a></h2>
<p>The general data science workflow for this project is shown in <a class="reference internal" href="#workflow"><span class="std std-numref">Fig. 3</span></a>. We discuss each step of the workflow in more detail in the sub-sections below.</p>
<div class="figure align-default" id="workflow">
<a class="reference internal image-reference" href="_images/ds_workflow.png"><img alt="_images/ds_workflow.png" src="_images/ds_workflow.png" style="height: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">General data science workflow for this capstone project.</span><a class="headerlink" href="#workflow" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="data-preparation">
<span id="data-prep"></span><h3>4.1 Data Preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="data-splitting-and-baseline-model">
<h4>4.1.1 Data Splitting and Baseline Model<a class="headerlink" href="#data-splitting-and-baseline-model" title="Permalink to this headline">¶</a></h4>
<p>Before beginning any model training, the data was split into train, validation and test splits of 70%, 15% and 15% respectively. Here, we define baseline as a model that only predicts negative and has 62% accuracy. This accuracy corresponds to the proportion of negatives in the dataset. Our goal for this project was to develop a model that performed better than baseline.</p>
</div>
<div class="section" id="image-augmentation">
<h4>4.1.2 Image Augmentation<a class="headerlink" href="#image-augmentation" title="Permalink to this headline">¶</a></h4>
<p>Given the small size of the dataset, image augmentation techniques were used to expand the size of the training set and improve the model’s generalizability. We explored the pytorch <span id="id15">[<a class="reference internal" href="#id38">Paszke <em>et al.</em>, 2019</a>]</span> and the albumentations libraries as they had proven successful in increasing accuracy in previous work <span id="id16">[<a class="reference internal" href="#id25">Al-Dhabyani <em>et al.</em>, 2019</a>]</span>. We also attempted to leverage a machine learning tool called autoalbument from the latter library that searches for and selects the best transformations. However, following several runs, the accuracy did not improve from baseline. We suspect it may be that the transformations were too complex for the model to learn anything significant. Using simpler transformations to augment the data was complemented by results found in our literature review <span id="id17">[<a class="reference internal" href="#id31">Esteva <em>et al.</em>, 2017</a>, <a class="reference internal" href="#id28">Loey <em>et al.</em>, 2020</a>]</span>. A variety of classic transformations were tested and the model’s performance on these augmented datasets were documented <a class="reference external" href="https://github.com/UBC-MDS/capstone-gdrl-lipo/blob/master/notebooks/manual-albumentation.ipynb">here</a>. The transformations that led to the best performance were adding random vertical and horizontal flipping along with random brightness and contrast adjustment, with a probability of 50%, augmenting the images as seen below in <a class="reference internal" href="#transformation"><span class="std std-numref">Fig. 4</span></a>.</p>
<div class="figure align-default" id="transformation">
<a class="reference internal image-reference" href="_images/transformed_image.png"><img alt="_images/transformed_image.png" src="_images/transformed_image.png" style="height: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Final image transformations included random vertical and horizontal flipping and random brightness and contrast adjustment.</span><a class="headerlink" href="#transformation" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="model-development">
<span id="model-dev"></span><h3>4.2 Model Development<a class="headerlink" href="#model-development" title="Permalink to this headline">¶</a></h3>
<p>The augmented data is then used to train a CNN model using transfer learning, a technique utilizing pre-trained models on thousands of images, which allows for training with our comparatively smaller dataset. Based on our literature review, the transfer learning architectures we chose to investigate were the following: VGG16, ResNet50, DenseNet169 and InceptionV3. Each of the models were incorporated with our small dataset, trained in separate experiments utilizing techniques to optimize the parameters of the model to maximize its ability to learn. To compare the performance of the different architectures, the team considered the accuracy and recall scores of the models when tested on our test set. When comparing the relative scores of accuracy shown in <a class="reference internal" href="#acc-chart"><span class="std std-numref">Fig. 5</span></a> and recall shown in <a class="reference internal" href="#recall-chart"><span class="std std-numref">Fig. 6</span></a>, DenseNet outperformed the other architectures. DenseNet has also proved successful in similar deep learning applications using small datasets <span id="id18">[<a class="reference internal" href="#id36">Zhang <em>et al.</em>, 2020</a>]</span>, which we suspect is due to its ability to reduce the parameters in a model.</p>
<div class="figure align-default" id="acc-chart">
<a class="reference internal image-reference" href="_images/model_acc_bar_chart.png"><img alt="_images/model_acc_bar_chart.png" src="_images/model_acc_bar_chart.png" style="height: 150px;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">All four models were tested on a holdout sample to produce these accuracy results.</span><a class="headerlink" href="#acc-chart" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="recall-chart">
<a class="reference internal image-reference" href="_images/model_recall_bar_chart.png"><img alt="_images/model_recall_bar_chart.png" src="_images/model_recall_bar_chart.png" style="height: 150px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">All four models were tested on a holdout sample to produce these recall results.</span><a class="headerlink" href="#recall-chart" title="Permalink to this image">¶</a></p>
</div>
<p>Furthermore, we conducted a <a class="reference external" href="https://github.com/UBC-MDS/capstone-gdrl-lipo/blob/master/notebooks/model_decision_making.ipynb">manual analysis</a> of tricky negative and positive images to evaluate the models’ performance using an interactive interface. This application, demonstrated below in <a class="reference internal" href="#true-pos-all-wrong"><span class="std std-numref">Fig. 7</span></a>, allowed us to compare how confident the models were when misclassifying images and showcased the strengths of the DenseNet architecture.</p>
<div class="figure align-default" id="true-pos-all-wrong">
<a class="reference internal image-reference" href="_images/true_pos_all_wrong.png"><img alt="_images/true_pos_all_wrong.png" src="_images/true_pos_all_wrong.png" style="height: 500px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">A manual inspection of tricky examples was conducted. Above, we have a true positive and although, all model are struggling, DenseNet is the least confident in its wrong prediction.</span><a class="headerlink" href="#true-pos-all-wrong" title="Permalink to this image">¶</a></p>
</div>
<p>In an effort to make the model more generalizable, we considered various techniques such as implementing dropout layers in our CNN structure, batch normalization and taking an ensemble approach. Furthermore, when identifying liphypertrophic masses, it is more detrimental for a true positive (areas where lipohypertrophy is present) to be falsely labelled as a negative. This is formally known as a false negative and thus, we also attempted to optimize recall, a score where a higher value indicates fewer false negatives. To find out more about these explorations and their outcomes, see <a class="reference internal" href="capstone-report.html#document-documentation"><span class="doc std std-doc">Appendix A - Documentation</span></a>.</p>
<p>Finally, the choice of our optimal model as DenseNet was further motivated by its relatively small computational size as seen in <a class="reference internal" href="#size-df"><span class="std std-numref">Fig. 8</span></a> below, and its compatibility with our data product, further discussed in section <a class="reference internal" href="#data-product"><span class="std std-ref">5. Data Product and Results</span></a>.</p>
<div class="figure align-default" id="size-df">
<a class="reference internal image-reference" href="_images/size_df.png"><img alt="_images/size_df.png" src="_images/size_df.png" style="height: 250px;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">Comparison of the file size of the different models revealed that DenseNet was the smallest model.</span><a class="headerlink" href="#size-df" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="object-detection">
<span id="obj-detect"></span><h3>4.3 Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this headline">¶</a></h3>
<p>Our next objective was to implement object detection into our pipeline, giving us the ability to identify the exact location of lipohypertrophy on unseen test images. To implement object detection using a popular framework called YOLO <span id="id19">[<a class="reference internal" href="#id37">Jocher <em>et al.</em>, 2020</a>]</span>, the team created bounding boxes around the location of the lipohypertrophy masses in the positive training images using the annotated ultrasound images as a guide. Next, using the YOLOv5 framework, the YOLOv5m model was trained for 200 epochs with an image size of 300 and a batch size of 8. The team experimented with different training parameters to find that the aforementioned training parameters produce optimal results. Below is a sample of those results identifying the exact location of lipohypertrophy with great confidence.</p>
<div class="figure align-default" id="obj-det">
<a class="reference internal image-reference" href="_images/object_detect_example.png"><img alt="_images/object_detect_example.png" src="_images/object_detect_example.png" style="height: 500px;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Our final object detection model results on a test sample reveals promising results. The top row indicates the true location of lipohypertrophy and the bottom row indicates where the model thinks the lipohypertrophy is. The number on the red box indicates the model’s confidence.</span><a class="headerlink" href="#obj-det" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="data-product-and-results">
<span id="data-product"></span><h2>5. Data Product and Results<a class="headerlink" href="#data-product-and-results" title="Permalink to this headline">¶</a></h2>
<p>Our project comprised of two key data products:</p>
<ol class="simple">
<li><p>Source Code</p></li>
<li><p>Web application</p></li>
</ol>
<p>We describe these products in detail below.</p>
<p>The first deliverable data product is the source code including an environmental file (.yaml), command-line enabled python scripts to perform train-validation-test split on image folders, modeling, evaluation, and deployment, and a Makefile that automates the entire process. The team expects our partner to use the source code to refit the lipohypertrophy classification model as more ultrasound images become available. The python scripts are built with optional arguments for model hyperparameters, which makes this a flexible tool to work with. The Makefile renders the process automatic and makes the model refitting effortless. However, the source code requires a certain degree of python and command line interface knowledge.</p>
<p>The second deliverable is a web application as shown in <a class="reference internal" href="#web-app"><span class="std std-numref">Fig. 10</span></a> for our partner to interact with the lipohypertrophy classification and object detection models. A web application was an ideal choice compared to a local desktop app since the former does not require internal IT clearance, making it more ideal for our partner. The team expects our partner to use this app to upload one or multiple cropped ultrasound images, and get an instant prediction of lipohypertrophy presence along with the prediction confidence. If the prediction is positive, a red box will appear on the image to indicate the proposed lipohypertrophy area. The app is deployed on Streamlit share and Heroku, two free-service deployment platforms. The team sent a special request form to Streamlit to increase the allocated resources under Streamlit’s “good for the world” projects program. The request was approved and the app deployed on Streamlit share runs much faster in comparison to that deployed on Heroku; which now serves as a backup. Future (larger) models could be hosted on cloud based servers since they are more flexible and secure.</p>
<div class="figure align-default" id="web-app">
<a class="reference internal image-reference" href="_images/demo_gif_v2.gif"><img alt="_images/demo_gif_v2.gif" src="_images/demo_gif_v2.gif" style="height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Our <a class="reference external" href="https://share.streamlit.io/xudongyang2/lipo_deployment/demo_v2.py">web application</a> on Streamlit can take multiple images and provides a final prediction and its confidence for a given image. If the model is considered positive, the model will also propose the location of the lipohypetrophy.</span><a class="headerlink" href="#web-app" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="conclusions-and-recommendations">
<span id="recommendations"></span><h2>6. Conclusions and recommendations<a class="headerlink" href="#conclusions-and-recommendations" title="Permalink to this headline">¶</a></h2>
<p>In this project, we aimed to investigate whether supervised machine learning techniques could be leveraged to detect the presence of lipohypertrophy in ultrasound images. Our trained models have demonstrated that this is indeed a possibility as they are accurately predicting the presence of lipohypertrophy on 76% of previously unseen data. Furthermore, our team has developed two data products. The data products are a well-documented source code and an automated machine learning pipeline and an interactive web application.</p>
<p>The open-source licensed source code will allow future researchers and developers to borrow from and build upon this work. The Makefile included with the project makes it seamless to update the model with an expanded dataset. The web application allows healthcare providers to easily interact with our machine learning model to discover which sites are safe for insulin injection and which sites should be avoided.</p>
<p>Although our project has demonstrated that machine learning can be used to detect lipohypertrophy, there are some key limitations that should be addressed before it is used in the clinical setting. These key limitations are as follows:</p>
<ol class="simple">
<li><p>Scarcity of data - We had merely 353 images to develop our model which is a small dataset in the realm of deep learning. The scarcity of our dataset caused variability in different runs of the experiments conducted. We have also noticed that our model’s performance is sensitive to data splitting (i.e. which images are in the training, validation and test folders).</p></li>
<li><p>Limited resources - Limited computing power has been a recurrent challenge in this project and has made it difficult to experiment with tuning all the parameters of our model concurrently.</p></li>
<li><p>Lack of oversight - We believe that there should be an auditing process involved to ensure that our machine learning model does not propagate any biases that could cause harm to specific patient populations. It would also be useful to have other independent radiologists label the dataset in order to draw comparisons between physicians’ and algorithms’ performance.</p></li>
</ol>
<p>To address the limitations outlined above, our team has made the following recommendations:</p>
<ol class="simple">
<li><p>Addition of more labelled images – in line with the size of the dataset in similar studies <span id="id20">[<a class="reference internal" href="#id29">Cheng and Malhi, 2017</a>, <a class="reference internal" href="#id30">Xiao <em>et al.</em>, 2018</a>]</span>, we recommend increasing the dataset to at least 1000 images and ideally up to several thousand images.</p></li>
<li><p>Increasing computational resources - we recommend obtaining funding for additional computing resources for both the development and deployment of future models.</p></li>
<li><p>Adding additional functionality such as a cropping tool within the web interface.</p></li>
</ol>
</div>
<div class="section" id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h2>
<p id="id21"><dl class="citation">
<dt class="label" id="id25"><span class="brackets">ADGKA19</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id16">2</a>)</span></dt>
<dd><p>Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Fahmy Aly. Deep learning approaches for data augmentation and classification of breast masses using ultrasound images. <em>Int. J. Adv. Comput. Sci. Appl</em>, 10(5):1–11, 2019.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id20">CM17</a></span></dt>
<dd><p>Phillip M Cheng and Harshawn S Malhi. Transfer learning with convolutional neural networks for classification of abdominal ultrasound images. <em>Journal of digital imaging</em>, 30(2):234–243, 2017.</p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id6">CCL+19</a></span></dt>
<dd><p>Jui-Ying Chiao, Kuan-Yung Chen, Ken Ying-Kai Liao, Po-Hsin Hsieh, Geoffrey Zhang, and Tzung-Chi Huang. Detection and classification the breast tumors using mask r-cnn on sonograms. <em>Medicine</em>, 2019.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">EKN+17</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. <em>Nature (London)</em>, 542(7639):115–118, 2017.</p>
</dd>
<dt class="label" id="id22"><span class="brackets"><a class="fn-backref" href="#id14">Gan18</a></span></dt>
<dd><p>Rohith Gandhi. R-cnn, fast r-cnn, faster r-cnn, yolo - object detection algorithms. Jul 2018. URL: <a class="reference external" href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a>.</p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id5">HZL18</a></span></dt>
<dd><p>Qinghua Huang, Fan Zhang, and Xuelong Li. Machine learning in ultrasound computer-aided diagnostic systems: a survey. <em>BioMed research international</em>, 2018.</p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id19">JSB+20</a></span></dt>
<dd><p><strong>missing booktitle in glenn_jocher_2020_4154370</strong></p>
</dd>
<dt class="label" id="id24"><span class="brackets"><a class="fn-backref" href="#id12">Kag16</a></span></dt>
<dd><p>Kaggle. Ultrasound nerve segmentation. 2016. URL: <a class="reference external" href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/overview">https://www.kaggle.com/c/ultrasound-nerve-segmentation/overview</a>.</p>
</dd>
<dt class="label" id="id23"><span class="brackets"><a class="fn-backref" href="#id13">Kag21</a></span></dt>
<dd><p>Kaggle. Vinbigdata chest x-ray abnormalities detection. 2021. URL: <a class="reference external" href="https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/code?competitionId=24800&amp;sortBy=voteCount">https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/code?competitionId=24800&amp;sortBy=voteCount</a>.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">KPCM18</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>,<a href="#id4">3</a>)</span></dt>
<dd><p>JE Kapeluto, BW Paty, SD Chang, and GS Meneilly. Ultrasound detection of insulin-induced lipohypertrophy in type 1 and type 2 diabetes. <em>Diabetic Medicine</em>, 35(10):1383–1390, 2018.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">LMK20</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>Mohamed Loey, Gunasekaran Manogaran, and Nour Eldeen M Khalifa. A deep transfer learning model with classical data augmentation and cgan to detect covid-19 from chest ct radiography digital images. <em>Neural Computing and Applications</em>, pages 1–13, 2020.</p>
</dd>
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id2">Mad21</a></span></dt>
<dd><p>Ken Madden. Machine learning approaches to: 1. diagnosing lipohypertrophy at the bedside, and 2. falls prediction in long term care. 2021. URL: <a class="reference external" href="https://github.ubc.ca/MDS-2020-21/DSCI_591_capstone-proj_students/blob/master/proposals/md/Machine_Learning_Approaches_to_Diagnosing_Lipohypertrophy_and_Predicting_Falls.md">https://github.ubc.ca/MDS-2020-21/DSCI_591_capstone-proj_students/blob/master/proposals/md/Machine_Learning_Approaches_to_Diagnosing_Lipohypertrophy_and_Predicting_Falls.md</a>.</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id15">PGM+19</a></span></dt>
<dd><p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: an imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle  Alché-Buc, E. Fox, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 32</em>, pages 8024–8035. Curran Associates, Inc., 2019. URL: <a class="reference external" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>.</p>
</dd>
<dt class="label" id="id35"><span class="brackets"><a class="fn-backref" href="#id10">SLZ+20</a></span></dt>
<dd><p>Qiuchang Sun, Xiaona Lin, Yuanshen Zhao, Ling Li, Kai Yan, Dong Liang, Desheng Sun, and Zhi-Cheng Li. Deep learning vs. radiomics for predicting axillary lymph node metastasis of breast cancer using ultrasound images: don't forget the peritumoral region. <em>Frontiers in oncology</em>, 10:53, 2020.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id20">XLL+18</a></span></dt>
<dd><p>Ting Xiao, Lei Liu, Kai Li, Wenjian Qin, Shaode Yu, and Zhicheng Li. Comparison of transferred deep neural networks in ultrasonic breast masses discrimination. <em>BioMed research international</em>, 2018.</p>
</dd>
<dt class="label" id="id34"><span class="brackets"><a class="fn-backref" href="#id7">YAAK18</a></span></dt>
<dd><p>Koichiro Yasaka, Hiroyuki Akai, Osamu Abe, and Shigeru Kiryu. Deep learning with convolutional neural network for differentiation of liver masses at dynamic contrast-enhanced ct: a preliminary study. <em>Radiology</em>, 286(3):887–896, 2018.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">ZHC+20</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id18">2</a>)</span></dt>
<dd><p>Heqing Zhang, Lin Han, Ke Chen, Yulan Peng, and Jiangli Lin. Diagnostic efficiency of the breast ultrasound computer-aided prediction model based on convolutional neural network in breast cancer. <em>Journal of Digital Imaging</em>, 33:1218–1223, 2020.</p>
</dd>
</dl>
</p>
</div>
<div class="toctree-wrapper compound">
<span id="document-documentation"></span><div class="section" id="appendix-a-documentation">
<h2>Appendix A - Documentation<a class="headerlink" href="#appendix-a-documentation" title="Permalink to this headline">¶</a></h2>
<p>In this section of the Appendix, we attempt to highlight at a high level, some of the technical decisions that were taken throughout the duration of this capstone project. This section presents some of the technical details behind those decisions.</p>
<div class="section" id="data-augmentation">
<h3>Data Augmentation<a class="headerlink" href="#data-augmentation" title="Permalink to this headline">¶</a></h3>
<p>The team initially used a web interface provided by the albumentations library, powered by PyTorch for image transformations, that allowed us to see what the different transformations would look like. Based on that, we chose some transformations to test in our pipeline. This experiment can be found <a class="reference external" href="https://github.com/UBC-MDS/capstone-gdrl-lipo/blob/master/notebooks/manual-albumentation.ipynb">here</a>. The final model was chosen based on the transformations yielding the highest accuracy score.</p>
<p>To use the albumentations library:</p>
<ul class="simple">
<li><p>This <a class="reference external" href="https://albumentations.ai/docs/examples/pytorch_classification/">guide</a> is extremely helpful for our classification task. You develop your own class that inherits from the torch dataset. To see an example of this implementation for our dataset, please check this <a class="reference external" href="https://github.com/UBC-MDS/capstone-gdrl-lipo/blob/master/additional_work/autoalbument/autoalbument_attempt2.ipynb">notebook</a>.</p></li>
</ul>
<p>We also tried a machine learning tool called <a class="reference external" href="https://albumentations.ai/docs/autoalbument/">autoalbument</a> which allows for the selection of optimal transformations.</p>
</div>
<div class="section" id="loss-functions">
<h3>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<p>A loss function, also known as cost function or error function, is a function that maps a set of parameter values for the network onto a scalar value. The scalar value is an indication of how well the parameters are completing their assigned tasks. In optimization problems, we seek to minimize the loss function.</p>
<p>The following functions were considered for the choice of the loss function used in the convolutional neural network model:</p>
<ol class="simple">
<li><p>BCEWithLogitsLoss</p></li>
<li><p>NLLLoss</p></li>
<li><p>Sum of Log Loss</p></li>
<li><p>Hinge</p></li>
<li><p>The mean absolute error</p></li>
</ol>
<p>Loss Function Implemeted in CNN Model:</p>
<ul class="simple">
<li><p>BCEWithLogitsLoss: Cross-Entropy loss <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss">(nn.BCEWithLogitsLoss)</a> combines a Sigmoid layer and the BCELoss in one single class. This loss function in often used in classification problems as a measure of reconstruction error. It uses log loss, which for logistic regression is a special case for cross-entropy loss for multi-class classification.</p></li>
</ul>
<p>Alternative Loss Functions Considered:</p>
<ul class="simple">
<li><p>NLLLoss: Negative Log-Likelihood Loss
This function expands the binary cross-entropy loss. It requires an additional logSoftMax layer in the last layer of the network. As it outputs probablities, they must sum to 1.
The function also has various reduction of output options such as the weighted mean of output or sum.</p></li>
<li><p>Sum of Log Loss: This function allows for the gradient to update with more incentive</p></li>
<li><p>Hinge: This function, nn.HingeEmbeddingLoss, is used more for measuring similarities. It tries to maximize the margin between decision boundary and data points. The main advantage is that the function penalizes incorrect predictions a lot but also correct ones that aren’t confident (less). That is, confident correct predictions are not penalized at all.</p></li>
<li><p>L1 MAE: The mean absolute error</p></li>
</ul>
</div>
<div class="section" id="structure-of-cnn-model">
<h3>Structure of CNN Model<a class="headerlink" href="#structure-of-cnn-model" title="Permalink to this headline">¶</a></h3>
<p>Based on our Liturature Review, the following transfer learning architectures were considered:</p>
<ol class="simple">
<li><p>DenseNet</p></li>
<li><p>Inception</p></li>
<li><p>VGG</p></li>
<li><p>ResNet</p></li>
</ol>
<p>The model variants that were considered are documented below, along with some notes thought to be relevant.</p>
<ul class="simple">
<li><p>VGG16: It makes the improvement over AlexNet by replacing large kernel-sized filters (<strong>11</strong> and <strong>5</strong> in the first and second convolutional layer, respectively) with multiple 3×3 kernel-sized filters one after another.</p></li>
<li><p>DenseNet121: DenseNet121 proved to be the best performing model given the Lipohypertrophy data. To see this exploration, click <a class="reference external" href="https://github.com/UBC-MDS/capstone-gdrl-lipo/blob/master/notebooks/model_decision_making.ipynb">here</a>.</p></li>
<li><p>ResNet50</p></li>
<li><p>InceptionV3</p></li>
</ul>
<div class="section" id="batch-normalization">
<h4>Batch Normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this headline">¶</a></h4>
<p>We ensured all models have batch normalization implemented. This will standardize the inputs of a network improving the overall efficiency by reducing the number of epochs required to train for a given model.</p>
</div>
<div class="section" id="dropout-layer">
<h4>Dropout Layer<a class="headerlink" href="#dropout-layer" title="Permalink to this headline">¶</a></h4>
<p>In an effort to reduce the generalization error and high confidence in misclassifications, we considered the implementation of dropout layers within the model’s architecture. Dropout layers will randomly drop nodes in the network such that the model learns more robustly and the validation performance is improved. A dropout rate can be specified to indicate the probablity at which nodes are dropped. The dropout layers experiments with varying dropout rates did not show any success for our dataset. Although the dropout layers managed to reduce misclassifications in the models, the overall reduced accuracy was not remarkable enough to implement those layers in our optimal model. To see this exploration, click <a class="reference external" href="https://github.com/UBC-MDS/capstone-gdrl-lipo/blob/master/notebooks/densemodels-ax-dropout-layers.ipynb">here</a>.</p>
</div>
<div class="section" id="ensemble-model">
<h4>Ensemble Model<a class="headerlink" href="#ensemble-model" title="Permalink to this headline">¶</a></h4>
<p>Another way to reduce the generalization error is to average the predictions from all four models, called an ensemble. However, an ensemble would not be feasible as it requires lots of resources such as enough CPU to train the models and in our case, since the models performed too similarily, the average accuracy would be lower.</p>
</div>
<div class="section" id="recall-optimization">
<h4>Recall Optimization<a class="headerlink" href="#recall-optimization" title="Permalink to this headline">¶</a></h4>
<p>To reduce the generalization error, we considered varying the pos_weight argument in the loss function. Positive weight (pos_weight) is one argument of the loss function used in our CNN model which, if greater than 1, prioritizes the positive examples more such that there is a heavier penalization (loss) on labelling the positive lipohypertrophy examples incorrectly. Increasing the positive weights to more than 1, we would expect the recall score to improve since there would be heavier penalization (loss) on positive examples, in an attempt to reduce false negatives (a true positive where the model predicts is negative). However, this method was not implemented in our final model as it proved to be unstable. After conducting a few experiments, we found high variance in the <a class="reference external" href="https://github.com/UBC-MDS/capstone-gdrl-lipo/blob/master/notebooks/pos-weight-exploration.ipynb">results</a>.</p>
</div>
</div>
<div class="section" id="hyperparameter-optimization">
<h3>Hyperparameter Optimization<a class="headerlink" href="#hyperparameter-optimization" title="Permalink to this headline">¶</a></h3>
<div class="section" id="bayesian-optimization-with-ax">
<h4>Bayesian Optimization with Ax<a class="headerlink" href="#bayesian-optimization-with-ax" title="Permalink to this headline">¶</a></h4>
<p>The general idea with the Bayesian method is that not every possible parameterization in the defined parameter space is explored. The process tunes parameters in a few iterations by building a surrogate model. A surrogate model is a probablistic model that uses an acquisition function to direct the next sample to make up configurations where an improvement over the current best parameterization is likely. Bayesian Optimization with Ax, a recently developed package for optimization by Facebook, relies on information from previous trials to propose better hyperparameters in the next evaluation. The Gaussian process is as follows:</p>
<ol class="simple">
<li><p>Build “smooth” surrogate model using Gaussian processes (initially Gaussian with mean 0 and variance equal to the noise in data. The surrogate model updates to a Gaussian model with mean equal to the estimated mean and updated variance based on the previous trial.</p></li>
<li><p>use the surrogate model to <strong>predict</strong> the next parameterization (estimated mean), out of the remaining ones and quantify uncertainty (updated estimated variance)</p></li>
<li><p>combine predictions and estimates to derive an acquisition function and then optimize it to find the best configuration.</p></li>
<li><p>Observe outcomes</p></li>
</ol>
<p>Fit new surrogate model and repeat process.</p>
<p>As a way to guide the surrogate model in the prediction of the next parameter configuration to explore, aquisition functions are utilizied.</p>
<ul class="simple">
<li><p>Three common examples include:</p></li>
</ul>
<ol class="simple">
<li><p>Probability of Improvement (PI).</p></li>
<li><p>Expected Improvement (EI).</p></li>
<li><p>Lower Confidence Bound (LCB)</p></li>
</ol>
<p>The most common technique used in Bayesian Optimization is the second choice: Expected Improvement. As such, that was used for our experiments found <a class="reference external" href="https://github.com/UBC-MDS/capstone-gdrl-lipo/blob/master/notebooks/densenet-optimized.ipynb">here</a>.</p>
<p>An Ax tutorial can be found <a class="reference external" href="https://ax.dev/versions/latest/tutorials/tune_cnn.html">here</a>.</p>
<p>The main advantages to Bayesian Optimization are:</p>
<ul class="simple">
<li><p>its ability to find better parameterizations with fewer iterations than grid search and;</p></li>
<li><p>striking a balance between exploration and exploitation where exploration refers to trying out parameterization with high uncertainty in the outcome and exploitation refers to the surrogate model predicting likely parameterizations.</p></li>
</ul>
<p>In addition to our Bayesian experiments, we consulted results in the following papers regarding classical transformations applied to small:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf">A Review of Bayesian Optimization</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1912.05686.pdf">Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax</a>.</p></li>
</ul>
</div>
</div>
</div>
<span id="document-glossary"></span><div class="section" id="appendix-b-glossary">
<h2>Appendix B - Glossary<a class="headerlink" href="#appendix-b-glossary" title="Permalink to this headline">¶</a></h2>
<p><strong>CPU:</strong> stands for for Central Processing Unit, is a portion of a computer that retrieves and executes instructions. CPU’s are used in this context to execute a sequence of stored instructions (commands).</p>
<p><strong>Ensemble Learning:</strong> the use of combining multiple learning algorithms to obtain improved predictive performance than any individual learning algorithm predictive performance. In this context, an ensemble approach could be achieved by averaging the predictions from multiple models.</p>
<p><strong>Epochs:</strong> a number defining the number of times that the learning algorithm will pass through the entire training dataset.</p>
<p><strong>Executable:</strong> a file or program with the ability to be run by a computer.</p>
<p><strong>False positive:</strong> a false positive, also referred to as type I error in Statistics, is an error in prediction where a negative example is misclassified as a positive example. In this context, a false positive is an indication of a Lipohypertrophy mass when there is none in reality.</p>
<p><strong>False negative:</strong> a false negative, also referred to as type II error in Statistics, is an error in prediction where a positive example is misclassified as a negative one. In this context, a false negative is an indication of a no Lipohypertrophy presence when there is in reality.</p>
<p><strong>GAN:</strong> short for Generative Adversarial Network, GANs are a machine learning model where two neural networks compete against each other. In this context, GANs can be used to develop authentic-looking but fake images of ultrasounds to build a large dataset that can be used for future applications.</p>
<p><strong>Gaussian process:</strong> a Gaussian process is a stochastic process, a collection of random variables with a time or space component, that allows finite collection of random variables to have a multivariate normal (bell-shaped) distribution. That is, every finite linear combination of random variables is normally distributed.</p>
<p><strong>RCNNs:</strong> short for recurrent correlation neural networks, RCNNs are a machine learning model used for object detection. RCNNs learn the data but dividing it into sections and taking the most important features to then classify.</p>
<p><strong>Radiomics:</strong> a technique using data characterization algorithms to extract a large number of features from radiographic medical images such as ultrasound images.</p>
<p><strong>Recall:</strong> a ratio representing the number of true positives divided by the number of true positives plus the number of false negatives. Optimizing recall means to reduce the number of false negatives.</p>
<p><strong>Transfer learning:</strong> a technique that allows us to leverage pre-existing models that have already been trained on thousands of images from various data sources and applying them to our dataset.</p>
<p><strong>Sigmoid layer:</strong> a sigmoid layer in a neural network applies a sigmoid function to the input such that the output is bounded in the interval (0,1).</p>
<p><strong>YOLO:</strong> short for You Only Look Once, YOLO is an object detection framework that uses convolutional neural networks to train on images and optimize detection performance in real-time.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ela Bandari, Lara Habashy, Javairia Raza, and Peter Yang<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>