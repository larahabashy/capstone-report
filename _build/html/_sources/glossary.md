## Appendix C - Glossary

CPU: stands for for Central Processing Unit, is a portion of a computer that retrieves and executes instructions. CPUâ€™s are used in this context to execute a sequence of stored instructions (commands). 

Ensemble Learning: The use of combining multiple learning algorithms to obtain improved predictive performance than any individual learning algorithm predictive performance. In this context, an ensemble approach could result from averaging the predictions from multiple models.

Epochs: a number defining the number of times that the learning algorithm will pass through the entire training dataset. 

Executable: a file or program with the ability to be run by a computer.

False positive: a false positive, also referred to as type I error in Statistics, is an error in prediction where a negative example is misclassified as a positive example. In this context, a false positive is an indication of a Lipohypertrophy mass when there is none in reality. 

False negative: a false negative, also referred to as type II error in Statistics, is an error in prediction where a positive example is misclassified as a negative one. In this context, a false negative is an indication of a no Lipohypertrophy presence when there is in reality. 

GAN: short for Generative Adversarial Network, GANs are a machine learning model where two neural networks compete against each other. In this context, GANs can be used to develop authentic-looking but fake images of ultrasounds to build a large dataset that can be used for future applications. 

Gaussian process: a Gaussian process is a stochastic process, a collection of random variables with a time or space component, that allows finite collection of random variables to have a multivariate normal (bell-shaped) distribution. That is, every finite linear combination of random variables is normally distributed.

RCNNs: short for recurrent correlation neural networks, RCNNS are a machine learning model used for object detection. RCNNs learn the data but dividing it into sections and taking the most important features to then classify.

Recall: a ratio representing the number of true positives divided by the number of true positives plus the number of false negatives. Optimizing recall means to reduce the number of false negatives.

Transfer learning : a technique that allows us to leverage pre-existing models that have already been trained on thousands of images from various data sources and applying them to our dataset.

Sigmoid layer: A sigmoid layer in a neural network applies a sigmoid function to the input such that the output is bounded in the interval (0,1).

YOLO: short for You Only Look Once, YOLO is an object detection framework that uses convolutional neural networks to train on images and optimize detection performance in real-time. 



